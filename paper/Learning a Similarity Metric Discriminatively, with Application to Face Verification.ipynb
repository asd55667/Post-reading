{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[原文](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的神经网络与SVM判别方法, 旨在学习数据的共同特征, 局限在类别数目少的分类中\n",
    "Siamese更适合在类别数目多, 每类的样本数量少\n",
    "\n",
    "早期, 基于PCA的EigenFace和基于LDA的FisherFace都是线性的, 即使是非线性的KPCA,KLDA也同样不能适应图像的几何变换, 无法学习出图像的不变特性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此文主要的思路是找到一个映射函数, 能在目标空间中度量输入空间的语义距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特点\n",
    "+ 判别式方法,学习相似性度量以匹配原型\n",
    "+ 超大类别分类\n",
    "+ 训练时不需要全部类别的样本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能量模型相较于传统概率模型的优势在于, 不需要在输入空间归一化概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Siamese网络$$\n",
    "![](./imgs/01siamese.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能量度量$E_W(X_1, X_2) = ||G_W(X_1)-G_W(X_2)||\\tag{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast loss\n",
    "\n",
    "$L(W)=\\sum_{i=1}^{P}L(W,(Y,X_1,X_2)^i)$\n",
    "\n",
    "\n",
    "$在优化(最小)L时, L_G应减少正样本的能量, L_I应增加负样本的能量\n",
    "\n",
    "$L(W,(Y,X_1,X_2)^i) = (1-Y)L_G(E_W(X_1,X_2)^i) + YL_I(E_W(X_1,X_2)^i)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cond 1.  $\\exists m>0$, $E_W^G+m<E_W^I$   \n",
    "### Cond 2. $H(E_W^G,E_W^I)$的最小值在 $E_W^G+m<E_W^I$ 半平面内\n",
    "### Cond 3. 在margin $E_W^G+m=E_W^I $ 上 H的负梯度与向量[-1,1]的点积为正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定理   令$H(E_W^G,E_W^I)$为$E_W^G$与$E_W^I$域内的凸函数,  且存在最小值; 假定存样本点的权重W满足Cond 1,  若Cond 3成立, 则存在W使H最小同时还能满足Cond 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总损失$H(E_W^G,E_W^I)=L_G(E_W^G)+L_I(E_W^I)\\tag{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由Cond 2且H为凸函数, margin上所有的负梯度均指向HP1半平面, \n",
    "\n",
    "假设在margin的$E_G^*$处取得最优(最小值), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E_G^*=argmin\\{H(E_W^G,E_W^G+m)\\}\\tag{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "故$HP_2任意点的损失均大于在E_G^*点损失$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(E_G^*)\\leq H(E_W^G,E_W^I)\\tag{4}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(E_G^*-\\epsilon, E_G^*+m+\\epsilon)由(E_G^*,E_G^*+m)向左向上平移而得到,H(E_G^*-\\epsilon, E_G^*+m+\\epsilon)在HP_1内$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(E_G^*-\\epsilon, E_G^*+m+\\epsilon)\\tag{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(E_G^*-\\epsilon, E_G^*+m+\\epsilon)$$\n",
    "\n",
    "$$=H(E_G^*,E_G^*+m)-\\epsilon \\frac{\\partial H}{\\partial E_W^G} + \\epsilon \\frac{\\partial H}{\\partial E_W^I} + O(\\epsilon^2)$$\n",
    "$=H(E_G^*,E_G^*+m)+\\epsilon\\left[\\frac{\\partial H}{\\partial E_W^G} \\frac{\\partial H}{\\partial E_W^I}\\right]\\left[\\begin{matrix}-1 \\\\ 1 \\end{matrix}\\right]+O(\\epsilon^2)\\tag{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H负梯度的点积为正, 故$\\left[\\frac{\\partial H}{\\partial E_W^G} \\frac{\\partial H}{\\partial E_W^I}\\right]\\left[\\begin{matrix}-1 \\\\ 1 \\end{matrix}\\right]$为正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(E_G^*-\\epsilon, E_G^*+m+\\epsilon)\\leq H(E_G^*,E_G^*+m)\\tag{7}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对单一样本的损失有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(W, Y, X_1, X_2) = (1-Y)L_G(E_W) + YL_I(E_W) = (1-Y)\\frac{2}{Q}(E_W)^2+(Y)2Qe^{\\frac{2.77}{Q}E_W}\\tag{8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相似性度量是对称的, A与B相似度应和B与A的相似度相同\n",
    "minimize(A==B)的同时 maximize(A!=B)\n",
    "\n",
    "正样本(相匹配)的相似度越高损失越低, 负样本(不相匹配)相似度越高损失越高\n",
    "\n",
    "训练的焦点集中在负样本且相似度较高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对相似性度量\n",
    "Contrast loss有\n",
    "\n",
    "$L = \\frac{1}{2N} \\sum_{n=1}^{N}yd^2 + (1-y)max(margin-d,0)^2$\n",
    "\n",
    "$d = ||a_n-b_n||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 应用领域 \n",
    "人脸识别, 人脸验证, 目标跟踪"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
