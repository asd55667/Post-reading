{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数的微小变动随着网络的深度增加而放大, BN通过改变输入的分布进而减少网络内部的协变量偏移, 有一定的正则效果\n",
    "\n",
    "# BatchNormalization\n",
    "- 减少了对参数初始化的依赖\n",
    "- 能以更大的学习率学习\n",
    "- 一些场合能避免使用Dropout\n",
    "+ 加速网络收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SGD通过优化网络参数\\Theta来得到最小的Loss$\n",
    "\\begin{equation}\n",
    "\\Theta = \\mathop \\arg\\min_{\\Theta} \\frac{1}{N}\\sum_{i=1}^{N}l(x_i, \\Theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于内部网络计算\n",
    "\n",
    "$l=F_2(F_1(u,\\Theta_1),\\Theta_2)$\n",
    "\n",
    "$\\Theta_2的学习可等效为l=F_2(x,\\Theta_2)$\n",
    "\n",
    "$输入x=F_1(u,\\Theta_1)的分布对\\Theta_2的学习至关重要$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早期的sigmoid函数在输入到达S曲线的顶部时梯度为零,使网络的训练需要将参数约束到S曲线的上升区间内,造成BP的梯度消失, 而这样的结果,使得输入的大多数维度进入饱和状态,延长了网络收敛.\n",
    "\n",
    "随后出现的应对方法是ReLu, Glorot/Xavier初始化, 使用较小小的学习率,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Internal Covariate Shift\n",
    "输入白化后网络训练能更快的收敛, 线性变换至0均值,1方差, 去相关.\n",
    "\n",
    "白化每一层的输入, 可以使输入保持一个固定的分布,来移除网络内部的协变量偏移.\n",
    "\n",
    "如果将白化添加到BP的优化过程中, bias在中心化时会被抵消, 而梯度的不断更新会导致bias无限增大, 会导致归一化后BP的优化失效.  \n",
    "\n",
    "对此, BP中我们需计算输入x以及对应的原始样本雅可比矩阵\n",
    "\n",
    "$\\frac{\\partial Norm(x,\\chi)}{\\partial x}和\\frac{\\partial Norm(x,\\chi)}{\\partial \\chi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Statistics via Mini-Batch\n",
    "完全白化每一层的输入代价非常大,而且不能保证处处可微,\n",
    "两步简化\n",
    "> 1. 单独归一化每一层的标量特征, 即使特征未去相关,仍然能加速收敛\n",
    "$$\\hat{x}^{(k)} = \\frac{x^(k)-E[x^{(k)}]}{\\sqrt{ Var[x^{(k)}]} }$$\n",
    "\n",
    "然而单纯的归一化可能会改变网络的表示能力, 如sigmoid函数的归一化会丧失其部分非线性变换\n",
    "\n",
    "通过引入额外的平移变换与尺度变换$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)}+\\beta^{(k)}$$\n",
    "\n",
    "其中使$\\gamma^{(k)}=\\sqrt{Var[x^{(k)}}, \\beta^{(k)}=E[x^{(k)}]$能还原原始的表示能力\n",
    "> 2. 取每个batch_size的均值和方差来进行BN\n",
    "\n",
    "相较于协方差的方法, 激活函数的白化数比batch_size要大,涉及到非奇异矩阵的计算, 需要添加正则项; 仅计算每个维度的方差要简单得多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加BN到网络中可得到任意的激活, \n",
    "\n",
    "$其中\\gamma与\\beta参数是可以通过学习得到的$\n",
    "在归一化时需添加一个常数,避免batch_size内的样本方差为0而分母为0\n",
    "\n",
    "$$\\hat{x}_i \\leftarrow \\frac{x_i-\\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Inference with Batch-Normalized Networks\n",
    "激活函数的归一化能达到有效的训练, 而在推理的过程中是不必要的\n",
    "\n",
    "在推理过程, 取所有Batch_size得到的均值和方差的平均, \n",
    "$E_B[\\mu_B]\\rightarrow E[x]$\n",
    "\n",
    "$\\frac{m}{m-1}E_B[\\sigma_B^2] \\rightarrow Var[x]$\n",
    "\n",
    "将BN层的变换公式替换成\n",
    "$$y=\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}x+(\\beta-\\frac{\\gamma E[x]}{\\sqrt{Var[x]+\\epsilon}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Normalized Convolutional Networks\n",
    "对网络中的z = g(Wu+b)\n",
    "\n",
    "u通常为上层的非线性输出,其分布在训练中可能发生改变,\n",
    "\n",
    "Wu+b更可能拥有对称,非稀疏分布, 对其进行归一化相当是给一稳定的分布进行线性激活, 其中b可以被忽略\n",
    "$z=(Wu+b)\\rightarrow z=(BN(Wu))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization enables higher learning rate\n",
    "\n",
    "- 能防止训练卡住在非线性饱和区\n",
    "- 对参数的尺度有更强的适应性, 能稳固参数的增长\n",
    "\n",
    "尺度变换不会影响到网络层的雅可比矩阵"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
